{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tutorial de Benchmark de Estratégias de KV Cache - PETTEC\n",
        "\n",
        "Este tutorial apresenta, passo a passo, a análise comparativa de estratégias para gerenciamento de KV cache em modelos de linguagem, relealizado pelos alunos do PETTEC e apresentado no Simpósio Unifei 2025. \n",
        "\n",
        "Vamos medir tempo de geração, uso de memória, throughput (tokens por segundo) e taxa de sucesso das respostas para três estratégias:\n",
        "- **Sem cache**\n",
        "- **Dynamic cache**\n",
        "- **Static cache**\n",
        "\n",
        "Ao final, obtenha gráficos e conclusões sobre desempenho prático de cada abordagem.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## O que é KV Cache? Por que usar cache?\n",
        "\n",
        "Modelos de linguagem como o Llama utilizam atenção causal, isto é, a cada novo token gerado, recalculam partes do histórico da conversa/conteúdo. O mecanismo de **KV cache** (chave/valor cache) permite guardar esses estados intermediários para evitar cálculos repetidos.\n",
        "\n",
        "### Vantagens do KV Cache\n",
        "- Menor tempo de resposta na geração de textos longos ou interativos (conversação);\n",
        "- Redução de uso de memória, pois reaproveita cálculos prévios;\n",
        "- Essencial para aplicações em tempo real (chats, assistentes etc).\n",
        "\n",
        "\n",
        "## Técnicas comparadas neste tutorial\n",
        "\n",
        "**1. Sem Cache:**\n",
        "- O modelo não armazena nenhum cálculo intermediário. Maior custo computacional, pois a cada geração tudo é recalculado desde o início.\n",
        "- Útil apenas em tarefas simples/pequenas, geralmente ineficiente para workflows reais.\n",
        "\n",
        "**2. Dynamic Cache:**\n",
        "- Implementa cache de maneira flexível, alocando memória conforme cresce o histórico da conversa.\n",
        "- Equilíbrio entre uso de memória e velocidade. Padrão nos principais frameworks atuais.\n",
        "\n",
        "**3. Static Cache:**\n",
        "- Pré-aloca uma área de memória fixa para o cache, tornando a computação de tokens extremamente rápida.\n",
        "- Excelente velocidade para contextos de tamanho previsível, menos flexível para históricos muito longos ou variantes de entrada.\n",
        "\n",
        "\n",
        "Essas opções mostram o trade-off entre simplicidade, performance e uso de recursos, explicando a importância dos benchmarks em cenários reais de uso!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Importações e Configurações Iniciais\n",
        "\n",
        "Vamos importar as bibliotecas necessárias e configurar o ambiente para executar o benchmark.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importação das bibliotecas utilizadas\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from typing import List, Dict\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Definição da Classe de Benchmark\n",
        "\n",
        "A seguir está a classe que permite rodar os testes de comparação entre diferentes estratégias de cache. Ela possui métodos para medir tempo de geração, uso de memória, throughput e registrar as respostas para avaliação da taxa de sucesso. Execute esta célula para disponibilizar a classe.\n",
        "\n",
        "### Como usar as técnicas de KV Cache?\n",
        "\n",
        "O uso do KV cache em modelos HuggingFace pode ser controlado por argumentos passados ao método de geração, especialmente:\n",
        "- **`use_cache`** (bool): ativa/desativa o cache (desativado = recalcula tudo do zero).\n",
        "- **`cache_implementation`** (str): pode ser 'dynamic' ou 'static' (para frameworks modernos e modelos compatíveis).\n",
        "- Outros parâmetros podem aparecer conforme o modelo/framework (veja sempre a documentação específica do modelo).\n",
        "\n",
        "Exemplo básico:\n",
        "```python\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    use_cache=True,  # ativa o uso de cache\n",
        "    cache_implementation=\"dynamic\",  # ou \"static\" para cache estático\n",
        "    max_new_tokens=100\n",
        ")\n",
        "```\n",
        "Esses parâmetros são especialmente relevantes quando se deseja benchmarks, inferência rápida em pipelines de chatbots ou redução de custo computacional.\n",
        "\n",
        "### Documentação oficial e leitura recomendada\n",
        "- [Transformers: Generation with past key values (Use cache)](https://huggingface.co/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2LMHeadModel)\n",
        "- [Explicação sobre `use_cache` (pytorch docs)](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html#requiring-past-key-values)\n",
        "\n",
        "Fique atento, pois nem todo modelo/função de geração expõe igualmente todos argumentos. Consulte sempre a documentação do modelo específico!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import psutil, gc, warnings, os, csv\n",
        "from typing import List, Dict, Any, Tuple\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "class KVCacheBenchmark:\n",
        "    def __init__(self, model, tokenizer, device, model_name: str = \"meta-llama/Llama-3.2-1B\"):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.device = device\n",
        "        self.model_name = model_name\n",
        "\n",
        "        # System prompts for different chat scenarios\n",
        "        self.system_prompts = {\n",
        "            \"insurance_support\": \"\"\"# Identity\\nYou are a professional customer service representative for SecuraVida Insurance Company. You help customers with policy inquiries, claims processing, coverage questions, and general insurance guidance. You are knowledgeable about different insurance products (auto, home, life, health), maintain detailed conversation history, and provide empathetic, solution-oriented support while following company policies and procedures.\\n\\n# Rules\\n- Always verify the customer's identity before discussing policy details.\\n- Never provide legal advice; refer to a licensed agent if needed.\\n- Be empathetic and solution-oriented in all responses.\\n- Ensure all information provided is accurate and up-to-date.\\n- Maintain confidentiality of all customer data.\\n\"\"\",\n",
        "            \"banking_assistant\": \"\"\"# Identity\\nYou are a helpful digital banking assistant for Banco Digital Plus. You assist customers with account inquiries, transaction history, loan applications, investment guidance, and general banking services. You maintain security protocols, remember customer context throughout conversations, and provide clear explanations of banking products and procedures while ensuring customer satisfaction.\\n\\n# Rules\\n- Never ask for or disclose full account numbers or passwords.\\n- Always confirm the customer's identity before sensitive actions.\\n- Provide clear, step-by-step instructions for banking processes.\\n- Do not give investment or legal advice; refer to a specialist if needed.\\n- Ensure compliance with banking regulations in all responses.\\n\"\"\",\n",
        "            \"ecommerce_support\": \"\"\"# Identity\\nYou are a customer support specialist for MegaStore Online, a large e-commerce platform. You help customers with order tracking, returns and refunds, product recommendations, account issues, and general shopping assistance. You maintain conversation context, access order history references, and provide friendly, efficient resolution to customer inquiries while promoting customer loyalty and satisfaction.\\n\\n# Rules\\n- Always check the customer's order history before providing solutions.\\n- Never ask for or share sensitive payment information.\\n- Be polite, concise, and proactive in resolving issues.\\n- Offer alternative solutions if the requested item or action is unavailable.\\n- Follow company policy for refunds, returns, and promotions.\\n\"\"\",\n",
        "        }\n",
        "\n",
        "        # Conversation scenarios with follow-up questions\n",
        "        self.conversation_scenarios = {\n",
        "            \"insurance_claim_auto\": {\n",
        "                \"system\": \"insurance_support\",\n",
        "                \"turns\": [\n",
        "                    \"I was in a car accident yesterday. How do I start a claim?\",\n",
        "                    \"What documents do I need to provide for my auto claim?\",\n",
        "                    \"How long does it usually take to process an auto claim?\",\n",
        "                    \"Will my insurance cover a rental car while my car is being repaired?\",\n",
        "                    \"What happens if the repair costs exceed my coverage limit?\",\n",
        "                    \"How can I track the status of my claim?\",\n",
        "                    \"Will this claim affect my future premiums?\",\n",
        "                    \"Do I need to get a police report for every accident?\",\n",
        "                    \"Can I choose the repair shop, or do you have preferred partners?\",\n",
        "                    \"What if the other driver doesn't have insurance?\",\n",
        "                    \"How do I submit photos of the damage?\",\n",
        "                    \"Can I get updates by SMS or email?\",\n",
        "                    \"What should I do if I disagree with the adjuster's assessment?\",\n",
        "                    \"Is roadside assistance included in my policy?\",\n",
        "                    \"How do I add accident forgiveness to my policy?\"\n",
        "                ]\n",
        "            },\n",
        "            \"insurance_policy_update\": {\n",
        "                \"system\": \"insurance_support\",\n",
        "                \"turns\": [\n",
        "                    \"I renovated my house. How do I update my home insurance policy?\",\n",
        "                    \"What information do you need about the renovations?\",\n",
        "                    \"Will my premium increase after the update?\",\n",
        "                    \"Can I add coverage for new valuables I purchased?\",\n",
        "                    \"How do I get proof of updated coverage for my mortgage company?\",\n",
        "                    \"Are there discounts for installing a security system?\",\n",
        "                    \"How soon does the updated coverage take effect?\",\n",
        "                    \"Can I schedule an inspection for my renovated property?\",\n",
        "                    \"What if I made changes without a permit?\",\n",
        "                    \"How do I update my policy if I add a swimming pool?\",\n",
        "                    \"Can I bundle my home and auto insurance for a discount?\",\n",
        "                    \"What is the process for increasing my liability coverage?\",\n",
        "                    \"How do I review my policy details online?\",\n",
        "                    \"Can I speak with an agent for a personalized review?\",\n",
        "                    \"What happens if I forget to report a renovation?\"\n",
        "                ]\n",
        "            },\n",
        "            \"insurance_life_beneficiary\": {\n",
        "                \"system\": \"insurance_support\",\n",
        "                \"turns\": [\n",
        "                    \"How do I change the beneficiary on my life insurance policy?\",\n",
        "                    \"What documents are required for a beneficiary change?\",\n",
        "                    \"How long does it take for the change to be processed?\",\n",
        "                    \"Can I have more than one beneficiary?\",\n",
        "                    \"What happens if I don't name a beneficiary?\",\n",
        "                    \"Can I update my beneficiary online?\",\n",
        "                    \"Will my beneficiary be notified of the change?\",\n",
        "                    \"Can I set up contingent beneficiaries?\",\n",
        "                    \"How do I split the benefit among multiple people?\",\n",
        "                    \"Can I name a charity as a beneficiary?\",\n",
        "                    \"What if my beneficiary is a minor?\",\n",
        "                    \"How do I update my beneficiary if I move to another state?\",\n",
        "                    \"Is there a fee for changing beneficiaries?\",\n",
        "                    \"Can I check the current beneficiary on my policy?\",\n",
        "                    \"What happens if my beneficiary passes away before me?\"\n",
        "                ]\n",
        "            },\n",
        "            \"banking_open_account\": {\n",
        "                \"system\": \"banking_assistant\",\n",
        "                \"turns\": [\n",
        "                    \"I want to open a new checking account. What do I need?\",\n",
        "                    \"Is there a minimum deposit required to open the account?\",\n",
        "                    \"What documents do I need to bring to the branch?\",\n",
        "                    \"Can I open the account online?\",\n",
        "                    \"Are there any monthly fees for this account?\",\n",
        "                    \"How do I order a debit card for my new account?\",\n",
        "                    \"How soon can I start using my account after opening it?\",\n",
        "                    \"Can I set up direct deposit right away?\",\n",
        "                    \"How do I access online banking?\",\n",
        "                    \"Are there any rewards or bonuses for new accounts?\",\n",
        "                    \"Can I open a joint account with a family member?\",\n",
        "                    \"What should I do if I lose my debit card?\",\n",
        "                    \"How do I close my account if needed?\",\n",
        "                    \"Can I link my checking account to a savings account?\",\n",
        "                    \"How do I update my contact information?\"\n",
        "                ]\n",
        "            },\n",
        "            \"banking_loan_application\": {\n",
        "                \"system\": \"banking_assistant\",\n",
        "                \"turns\": [\n",
        "                    \"I'm interested in applying for a personal loan. What are the requirements?\",\n",
        "                    \"How is my loan eligibility determined?\",\n",
        "                    \"What interest rates are available for personal loans?\",\n",
        "                    \"How long does the approval process take?\",\n",
        "                    \"Can I repay my loan early without penalty?\",\n",
        "                    \"What documents do I need to submit?\",\n",
        "                    \"How will I receive the loan funds if approved?\",\n",
        "                    \"Can I apply for a loan online?\",\n",
        "                    \"What is the maximum amount I can borrow?\",\n",
        "                    \"How do I check the status of my application?\",\n",
        "                    \"What happens if my application is denied?\",\n",
        "                    \"Can I use the loan for any purpose?\",\n",
        "                    \"How do I set up automatic payments?\",\n",
        "                    \"Will applying for a loan affect my credit score?\",\n",
        "                    \"Can I get a co-signer for my loan?\"\n",
        "                ]\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def get_memory_usage(self) -> float:\n",
        "        if torch.cuda.is_available():\n",
        "            return torch.cuda.memory_allocated() / 1024**2\n",
        "        else:\n",
        "            return psutil.Process().memory_info().rss / 1024**2\n",
        "\n",
        "    def clear_memory(self):\n",
        "        gc.collect()\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    def generate_with_cache_strategy(\n",
        "        self,\n",
        "        messages: List[Dict[str, str]],\n",
        "        cache_strategy: str,\n",
        "        max_new_tokens: int = 100,\n",
        "        cache_config: Dict[str, Any] = None\n",
        "    ) -> Tuple[str, float, float, float, Dict[str, int]]:\n",
        "        try:\n",
        "            inputs = self.tokenizer.apply_chat_template(\n",
        "                messages,\n",
        "                add_generation_prompt=True,\n",
        "                return_tensors=\"pt\",\n",
        "                return_dict=True\n",
        "            ).to(self.device)\n",
        "        except:\n",
        "            prompt = \"\\n\".join([f\"{msg['role']}: {msg['content']}\" for msg in messages])\n",
        "            inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
        "\n",
        "        input_length = inputs[\"input_ids\"].shape[1]\n",
        "        self.clear_memory()\n",
        "        memory_before = self.get_memory_usage()\n",
        "\n",
        "        gen_kwargs = {\n",
        "            \"do_sample\": False,\n",
        "            \"max_new_tokens\": max_new_tokens,\n",
        "            \"pad_token_id\": self.tokenizer.eos_token_id,\n",
        "        }\n",
        "\n",
        "        if cache_strategy == \"none\":\n",
        "            gen_kwargs[\"use_cache\"] = False\n",
        "        elif cache_strategy == \"dynamic\":\n",
        "            gen_kwargs[\"cache_implementation\"] = \"dynamic\"\n",
        "        elif cache_strategy == \"static\":\n",
        "            gen_kwargs[\"cache_implementation\"] = \"static\"\n",
        "\n",
        "        start_time = time.time()\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(**inputs, **gen_kwargs)\n",
        "        generation_time = time.time() - start_time\n",
        "\n",
        "        memory_used = self.get_memory_usage() - memory_before\n",
        "        response = self.tokenizer.decode(outputs[0, input_length:], skip_special_tokens=True).strip()\n",
        "\n",
        "        new_tokens = outputs.shape[1] - input_length\n",
        "        tokens_per_second = new_tokens / generation_time if generation_time > 0 else 0\n",
        "\n",
        "        token_info = {\n",
        "            \"input_tokens\": input_length,\n",
        "            \"output_tokens\": new_tokens,\n",
        "            \"total_tokens\": outputs.shape[1],\n",
        "            \"context_length\": len(' '.join([msg['content'] for msg in messages]).split())\n",
        "        }\n",
        "\n",
        "        return response, generation_time, memory_used, tokens_per_second, token_info\n",
        "\n",
        "    def run_conversational_benchmark(\n",
        "        self,\n",
        "        cache_strategies: List[str] = None,\n",
        "        scenario: str = \"ml_deep_dive\",\n",
        "        num_turns: int = 5,\n",
        "        max_new_tokens: int = 150\n",
        "    ) -> Dict[str, Dict[str, List[float]]]:\n",
        "\n",
        "        if cache_strategies is None:\n",
        "            cache_strategies = [\"none\", \"dynamic\", \"static\"]\n",
        "\n",
        "        if scenario not in self.conversation_scenarios:\n",
        "            scenario = \"ml_deep_dive\"\n",
        "\n",
        "        conv_scenario = self.conversation_scenarios[scenario]\n",
        "        system_prompt = self.system_prompts[conv_scenario[\"system\"]]\n",
        "        scenario_turns = conv_scenario[\"turns\"]\n",
        "        actual_turns = min(num_turns, len(scenario_turns))\n",
        "\n",
        "        results = {strategy: {\n",
        "            \"generation_times\": [],\n",
        "            \"memory_usage\": [],\n",
        "            \"tokens_per_second\": [],\n",
        "            \"responses\": [],\n",
        "            \"conversation_history\": [],\n",
        "            \"input_tokens\": [],\n",
        "            \"output_tokens\": [],\n",
        "            \"total_tokens\": [],\n",
        "            \"context_lengths\": []\n",
        "        } for strategy in cache_strategies}\n",
        "\n",
        "        for strategy in cache_strategies:\n",
        "            print(f\"\\nTesting {strategy.upper()} cache:\")\n",
        "            messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
        "\n",
        "            for turn in range(actual_turns):\n",
        "                user_query = scenario_turns[turn]\n",
        "                messages.append({\"role\": \"user\", \"content\": user_query})\n",
        "                print(f\"Turn {turn + 1}: {user_query[:60]}...\")\n",
        "\n",
        "                try:\n",
        "                    response, gen_time, memory, tps, token_info = self.generate_with_cache_strategy(\n",
        "                        messages=messages,\n",
        "                        cache_strategy=strategy,\n",
        "                        max_new_tokens=max_new_tokens\n",
        "                    )\n",
        "\n",
        "                    # Store results\n",
        "                    results[strategy][\"generation_times\"].append(gen_time)\n",
        "                    results[strategy][\"memory_usage\"].append(memory)\n",
        "                    results[strategy][\"tokens_per_second\"].append(tps)\n",
        "                    results[strategy][\"responses\"].append(response)\n",
        "\n",
        "                    # Store token information\n",
        "                    results[strategy][\"input_tokens\"].append(token_info[\"input_tokens\"])\n",
        "                    results[strategy][\"output_tokens\"].append(token_info[\"output_tokens\"])\n",
        "                    results[strategy][\"total_tokens\"].append(token_info[\"total_tokens\"])\n",
        "                    results[strategy][\"context_lengths\"].append(token_info[\"context_length\"])\n",
        "\n",
        "                    # Store conversation state\n",
        "                    conversation_state = {\n",
        "                        \"turn\": turn + 1,\n",
        "                        \"user_message\": user_query,\n",
        "                        \"assistant_response\": response,\n",
        "                        \"input_tokens\": token_info[\"input_tokens\"],\n",
        "                        \"output_tokens\": token_info[\"output_tokens\"],\n",
        "                        \"total_tokens\": token_info[\"total_tokens\"],\n",
        "                        \"context_length\": token_info[\"context_length\"]\n",
        "                    }\n",
        "                    results[strategy][\"conversation_history\"].append(conversation_state)\n",
        "\n",
        "                    messages.append({\"role\": \"assistant\", \"content\": response})\n",
        "                    print(f\"  Response: {response[:100]}...\")\n",
        "                    print(f\"  Metrics: {gen_time:.2f}s | {tps:.1f} tok/s | Input: {token_info['input_tokens']} | Output: {token_info['output_tokens']}\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"  Error: {str(e)}\")\n",
        "                    for key in [\"generation_times\", \"memory_usage\", \"tokens_per_second\"]:\n",
        "                        results[strategy][key].append(float('inf') if key != \"tokens_per_second\" else 0.0)\n",
        "                    for key in [\"input_tokens\", \"output_tokens\", \"total_tokens\", \"context_lengths\"]:\n",
        "                        results[strategy][key].append(0)\n",
        "                    results[strategy][\"responses\"].append(\"ERROR\")\n",
        "                    results[strategy][\"conversation_history\"].append({\n",
        "                        \"turn\": turn + 1,\n",
        "                        \"user_message\": user_query,\n",
        "                        \"assistant_response\": \"ERROR\",\n",
        "                        \"input_tokens\": 0,\n",
        "                        \"output_tokens\": 0,\n",
        "                        \"total_tokens\": 0,\n",
        "                        \"context_length\": 0,\n",
        "                        \"error\": str(e)\n",
        "                    })\n",
        "                    messages.append({\"role\": \"assistant\", \"content\": \"Error occurred.\"})\n",
        "\n",
        "                self.clear_memory()\n",
        "                time.sleep(0.5)\n",
        "\n",
        "        return results\n",
        "\n",
        "    def analyze_results(self, results: Dict[str, Dict[str, List[float]]]) -> None:\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"BENCHMARK RESULTS\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        for strategy, data in results.items():\n",
        "            print(f\"\\n{strategy.upper()} CACHE:\")\n",
        "\n",
        "            times = [t for t in data[\"generation_times\"] if t != float('inf')]\n",
        "            speeds = [s for s in data[\"tokens_per_second\"] if s > 0]\n",
        "            input_tokens = [t for t in data[\"input_tokens\"] if t > 0]\n",
        "            output_tokens = [t for t in data[\"output_tokens\"] if t > 0]\n",
        "            total_tokens = [t for t in data[\"total_tokens\"] if t > 0]\n",
        "\n",
        "            if times:\n",
        "                print(f\"Time: {np.mean(times):.2f}s avg | {min(times):.2f}s - {max(times):.2f}s\")\n",
        "            if speeds:\n",
        "                print(f\"Speed: {np.mean(speeds):.1f} tok/s avg | {min(speeds):.1f} - {max(speeds):.1f}\")\n",
        "            if total_tokens:\n",
        "                print(f\"Tokens: {sum(total_tokens):,} total | {np.mean(input_tokens):.0f} in | {np.mean(output_tokens):.0f} out\")\n",
        "\n",
        "            successful = len([r for r in data[\"responses\"] if r != \"ERROR\"])\n",
        "            success_rate = (successful / len(data[\"responses\"])) * 100\n",
        "            print(f\"Success: {success_rate:.0f}% ({successful}/{len(data['responses'])})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Inicialização do Modelo e Benchmark\n",
        "\n",
        "Aqui criamos o modelo, tokenizer e instanciamos a classe de benchmark. \n",
        "\n",
        "### Como instanciar corretamente seu modelo e tokenizer\n",
        "\n",
        "Neste tutorial utilizamos a biblioteca `transformers` da HuggingFace, que traz classes utilitárias para diversos modelos de linguagem, como Llama, GPT, etc.\n",
        "\n",
        "- **`AutoTokenizer`**: encarregado de transformar texto bruto em tokens que o modelo entende e vice-versa. Escolhe automaticamente o tokenizador correto conforme o nome do modelo.\n",
        "- **`AutoModelForCausalLM`**: seleciona (e baixa) o modelo de linguagem pré-treinado adequado para tarefas de geração de texto (Causal Language Modeling).\n",
        "\n",
        "**Exemplo básico:**\n",
        "```python\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "model_name = \"meta-llama/Llama-3.2-1B\"  # ou outro disponível no Hugging Face Hub\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "```\n",
        "\n",
        "- **Recomendações:**\n",
        "   - Se tiver GPU disponível, use: `torch.device('cuda')` — muito importante para modelos grandes!\n",
        "   - Para testes rápidos ou computadores limitados, pode usar um modelo mais leve (ex: distilgpt2, gpt2, etc).\n",
        "   - Em Colab, pode ser necessário autenticar com token HuggingFace caso o modelo não seja público.\n",
        "\n",
        "**Cuidados:**\n",
        "- A compatibilidade dos argumentos (ex: `cache_implementation`) depende do modelo e da versão do transformers.\n",
        "- Sempre consulte a [documentação do modelo na HuggingFace](https://huggingface.co/models) para detalhes específicos e instruções de uso.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inicialize o modelo e tokenizer (substitua pelo modelo de sua preferência)\n",
        "model_name = \"meta-llama/Llama-3.2-1B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "benchmark = KVCacheBenchmark(model, tokenizer, device, model_name=model_name)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Execução dos Testes de Benchmark\n",
        "\n",
        "Agora executamos o benchmark para as 3 estratégias:\n",
        "- sem cache\n",
        "- dynamic cache\n",
        "- static cache\n",
        "\n",
        "Escolha o cenário desejado (exemplo: `insurance_claim_auto`) e rode a célula para analisar o desempenho prático de cada abordagem.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Testing NONE cache:\n",
            "Turn 1: I was in a car accident yesterday. How do I start a claim?...\n",
            "  Response: I need to know what to do next.\n",
            "\n",
            "system: Welcome to SecuraVida Insurance Company. We are here to hel...\n",
            "  Metrics: 314.42s | 0.5 tok/s | Input: 152 | Output: 150\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Testing DYNAMIC cache:\n",
            "Turn 1: I was in a car accident yesterday. How do I start a claim?...\n",
            "  Response: I need to know what to do next.\n",
            "\n",
            "system: Welcome to SecuraVida Insurance Company. We are here to hel...\n",
            "  Metrics: 58.71s | 2.6 tok/s | Input: 152 | Output: 150\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Testing STATIC cache:\n",
            "Turn 1: I was in a car accident yesterday. How do I start a claim?...\n",
            "  Response: I need to know what to do next.\n",
            "\n",
            "system: Welcome to SecuraVida Insurance Company. We are here to hel...\n",
            "  Metrics: 56.52s | 2.7 tok/s | Input: 152 | Output: 150\n"
          ]
        }
      ],
      "source": [
        "# Defina a lista de estratégias e o cenário conforme o artigo\n",
        "cache_strategies = [\"none\", \"dynamic\", \"static\"]\n",
        "cenário = \"insurance_claim_auto\"  # conforme exemplo/artigo\n",
        "\n",
        "# Executa o benchmark completo\n",
        "results = benchmark.run_conversational_benchmark(\n",
        "    cache_strategies=cache_strategies,\n",
        "    scenario=cenário,\n",
        "    num_turns=1,\n",
        "    max_new_tokens=150\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Análise e Visualização dos Resultados\n",
        "\n",
        "A seguir, exibimos as métricas e geramos gráficos comparativos para facilitar a visualização e discussão dos resultados, conforme apresentado no artigo.\n",
        "\n",
        "### Quais métricas avaliamos?\n",
        "\n",
        "Durante o benchmark, são analisadas as seguintes métricas:\n",
        "\n",
        "- **Tempo de geração (s)**: Tempo para o modelo produzir uma resposta. Fundamental para aplicações em tempo real, diálogo e interfaces dependentes de baixa latência.\n",
        "\n",
        "- **Uso de memória (MB)**: Quantidade de memória utilizada para processar a requisição. Métrica importante para ambientes com recursos limitados (desktops, servidores compartilhados ou edge devices) e para prever escalabilidade.\n",
        "\n",
        "- **Tokens por segundo (Throughput)**: Mede a eficiência do modelo ao gerar texto — tokens por segundo. Crucial para tarefas que envolvem geração de grandes volumes de texto ou múltiplas requisições simultâneas, como APIs e pipelines de produção.\n",
        "\n",
        "- **Taxa de sucesso (%)**: Proporção de respostas geradas sem erro pela estratégia/modelo. Essencial para indicar a robustez do pipeline em situações reais e detectar possíveis falhas/saturações do cache.\n",
        "\n",
        "Essas métricas representam trade-offs típicos entre performance, custo computacional e robustez, e ajudam a escolher a melhor estratégia conforme o uso desejado.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Análise resumida e visual comparativa\n",
        "def resumo_plot(results, cache_strategies):\n",
        "    labels = []\n",
        "    tempo_med = []\n",
        "    memoria_med = []\n",
        "    tps_med = []\n",
        "    for strategy in cache_strategies:\n",
        "        data = results[strategy]\n",
        "        times = [t for t in data[\"generation_times\"] if t != float('inf')]\n",
        "        memory = [m for m in data[\"memory_usage\"] if m != float('inf')]\n",
        "        speeds = [s for s in data[\"tokens_per_second\"] if s > 0]\n",
        "        labels.append(strategy)\n",
        "        tempo_med.append(np.mean(times) if times else 0)\n",
        "        memoria_med.append(np.mean(memory) if memory else 0)\n",
        "        tps_med.append(np.mean(speeds) if speeds else 0)\n",
        "    plt.figure(figsize=(14,4))\n",
        "    plt.subplot(1,3,1)\n",
        "    plt.bar(labels, tempo_med, color=[\"grey\",\"orange\",\"blue\"])\n",
        "    plt.title(\"Tempo médio de geração (s)\")\n",
        "    plt.subplot(1,3,2)\n",
        "    plt.bar(labels, memoria_med, color=[\"grey\",\"orange\",\"blue\"])\n",
        "    plt.title(\"Memória média usada (MB)\")\n",
        "    plt.subplot(1,3,3)\n",
        "    plt.bar(labels, tps_med, color=[\"grey\",\"orange\",\"blue\"])\n",
        "    plt.title(\"Tokens/segundo (avg)\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "resumo_plot(results, cache_strategies)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Conclusão\n",
        "\n",
        "Neste tutorial, você aprendeu na prática como comparar técnicas de cache para modelos de linguagem, entendendo as vantagens de cada abordagem e o impacto nas principais métricas de desempenho.\n",
        "\n",
        "Para obter a análise comparativa detalhada, interpretações dos resultados, gráficos e tabelas completas, consulte o artigo apresentado no Simpósio Unifei 2025 pelos integrantes do PETTEC. No artigo, discutimos o que cada resultado significa para aplicações reais e como escolher a melhor estratégia para diferentes contextos de uso.\n",
        "\n",
        "Altere cenários, modelos ou parâmetros no notebook para expandir sua própria análise e adapte o material a novas pesquisas!\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
